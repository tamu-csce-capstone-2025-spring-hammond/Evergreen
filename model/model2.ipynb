{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa529b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.2.5)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (79.0.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 torch-2.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a708fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca50507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00546714 -0.00070611  0.00837226]\n",
      " [ 0.0157303  -0.00147323 -0.00220964]\n",
      " [ 0.01629213  0.00653948 -0.00503369]\n",
      " ...\n",
      " [ 0.00248948  0.00389391  0.00545154]\n",
      " [ 0.01285782 -0.00816843  0.00876259]\n",
      " [ 0.01242508 -0.0138287   0.00443582]]\n"
     ]
    }
   ],
   "source": [
    "returns = np.load('etf_returns.npy')\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d487ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sharpe(returns):\n",
    "    # Approximate Sharpe: mean / std\n",
    "    return returns.mean() / (returns.std() + 1e-6)\n",
    "\n",
    "def compute_drawdown(portfolio_values):\n",
    "    peak = np.maximum.accumulate(portfolio_values)\n",
    "    drawdowns = (peak - portfolio_values) / peak\n",
    "    return drawdowns.max()\n",
    "\n",
    "class FactorEnv:\n",
    "    \"\"\"\n",
    "    Simple gym-like environment for factor ETFs: SPMO, VOV, AVUV.\n",
    "    state: recent returns for each ETF\n",
    "    action: weight vector [w1, w2, w3]\n",
    "    reward: incremental portfolio return penalized by drawdown violation\n",
    "    \"\"\"\n",
    "    def __init__(self, returns, max_dd=0.1, window=20):\n",
    "        self.returns = returns  # shape [T, 3]\n",
    "        self.max_dd = max_dd\n",
    "        self.window = window\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = self.window\n",
    "        self.portfolio = 1.0\n",
    "        self.history = [self.portfolio]\n",
    "        return self.returns[self.t-self.window:self.t]\n",
    "\n",
    "    def step(self, action):\n",
    "        r = self.returns[self.t]\n",
    "        port_ret = (action * r).sum()\n",
    "        self.portfolio *= (1 + port_ret)\n",
    "        self.history.append(self.portfolio)\n",
    "        # compute reward as return minus penalty if drawdown > max\n",
    "        dd = compute_drawdown(np.array(self.history))\n",
    "        penalty = 1.0 if dd > self.max_dd else 0.0\n",
    "        reward = port_ret - penalty * 0.1\n",
    "        self.t += 1\n",
    "        done = (self.t >= len(self.returns))\n",
    "        next_state = None if done else self.returns[self.t-self.window:self.t]\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "class ExpertNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),  # scalar tilt\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # returns a scalar in [-1,1]\n",
    "        return self.net(x)\n",
    "\n",
    "class GatingNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, hidden_dim=64, pref_bias=None):\n",
    "        super().__init__()\n",
    "        self.pref_bias = pref_bias if pref_bias is not None else torch.zeros(num_experts)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_experts)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x) + self.pref_bias\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class MoEAgent:\n",
    "    def __init__(self, state_dim, num_experts=3, lr=1e-3, pref_bias=None):\n",
    "        self.experts = nn.ModuleList([ExpertNet(state_dim) for _ in range(num_experts)])\n",
    "        self.gate = GatingNet(state_dim, num_experts, pref_bias=pref_bias)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.gate.parameters()) + [p for e in self.experts for p in e.parameters()]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        gates = self.gate(state)\n",
    "        tilts = torch.stack([e(state) for e in self.experts], dim=-1).squeeze(-2)\n",
    "        # combine tilts to weights, ensure non-negative & sum to 1\n",
    "        raw_w = gates * (tilts + 1)  # map tanh to [0,2]\n",
    "        weights = raw_w / raw_w.sum(dim=-1, keepdim=True)\n",
    "        return weights, gates, tilts\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # Placeholder for policy gradient update using sampled trajectories\n",
    "        # Each trajectory: list of (state, action, reward)\n",
    "        # Compute loss = -expected return (or risk-adjusted metric)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.tensor(0.0)\n",
    "        # ... compute loss from trajectories ...\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b62a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     18\u001b[39m         state = torch.tensor(next_s, dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m final_port = env.history[-\u001b[32m1\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Final portfolio value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_port\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mMoEAgent.update\u001b[39m\u001b[34m(self, trajectories)\u001b[39m\n\u001b[32m     95\u001b[39m loss = torch.tensor(\u001b[32m0.0\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# ... compute loss from trajectories ...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/Evergreen/model/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/Evergreen/model/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/Evergreen/model/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load your historical ETF returns: shape [T,3]\n",
    "    returns = np.load('etf_returns.npy')  # columns: SPMO, VOV, AVUV\n",
    "    env = FactorEnv(returns)\n",
    "    agent = MoEAgent(state_dim=env.window*3, pref_bias=torch.tensor([0.5, 0.2, 0.3]))\n",
    "\n",
    "    num_episodes = 100\n",
    "    for ep in range(num_episodes):\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        while not done:\n",
    "            weights, gates, tilts = agent.get_action(state)\n",
    "            action = weights.squeeze(0).detach().numpy()\n",
    "            next_s, reward, done, _ = env.step(action)\n",
    "            trajectory.append((state, weights, reward))\n",
    "            if not done:\n",
    "                state = torch.tensor(next_s, dtype=torch.float32).unsqueeze(0)\n",
    "        agent.update(trajectory)\n",
    "        final_port = env.history[-1]\n",
    "        print(f\"Episode {ep+1}: Final portfolio value {final_port:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
